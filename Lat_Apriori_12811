{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activate_codes_num = -1\n",
    "next_k_step = 1\n",
    "training_chunk = 0\n",
    "test_chunk = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_history(data_history,training_key_set,output_size):\n",
    "    sum_history = {}\n",
    "    for key in training_key_set:\n",
    "        sum_vector = np.zeros(output_size)\n",
    "        count = 0\n",
    "        for lst in data_history[key]:\n",
    "            vec = np.zeros(output_size)\n",
    "            for ele in lst:\n",
    "                vec[ele] = 1\n",
    "            if vec[-2] == 1 or vec[-1] == 1:\n",
    "                continue\n",
    "            sum_vector += vec\n",
    "            count += 1\n",
    "        sum_vector = sum_vector / count\n",
    "        sum_history[key] = sum_vector\n",
    "    return sum_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_decay_add_history(data_set, key_set, output_size,within_decay_rate):\n",
    "    sum_history = {}\n",
    "    for key in key_set:\n",
    "        vec_list = data_set[key]\n",
    "        num_vec = len(vec_list) - 2\n",
    "        his_list = np.zeros(output_size)\n",
    "        for idx in range(1,num_vec+1):\n",
    "            his_vec = np.zeros(output_size)\n",
    "            decayed_val = np.power(within_decay_rate,num_vec-idx)\n",
    "            for ele in vec_list[idx]:\n",
    "                his_vec[ele] = decayed_val\n",
    "            his_list += his_vec\n",
    "        sum_history[key] = his_list/num_vec\n",
    "        # sum_history[key] = np.multiply(his_list / num_vec, IDF)\n",
    "\n",
    "    return sum_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(query_set, target_set, k):\n",
    "    history_mat = []\n",
    "    for key in target_set.keys():\n",
    "        history_mat.append(target_set[key])\n",
    "    test_mat = []\n",
    "    for key in query_set.keys():\n",
    "        test_mat.append(query_set[key])\n",
    "    # print('Finding k nearest neighbors...')\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(history_mat)\n",
    "    distances, indices = nbrs.kneighbors(test_mat)\n",
    "    # print('Finish KNN search.' )\n",
    "    return indices,distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_aggragate_outputs(data_chunk,training_key_set,index,distance,output_size):\n",
    "    output_vectors = []\n",
    "    key_set = training_key_set\n",
    "    for index_list_id in range(len(index)):\n",
    "        outputs = []\n",
    "        for vec_idx in range(1,next_k_step+1):\n",
    "\n",
    "            target_vec_list = []\n",
    "            weight_list = []\n",
    "            for id in range(len(index[index_list_id])):\n",
    "                dis = distance[index_list_id][id]\n",
    "                if dis == 0:\n",
    "                    weight_list.append(0)\n",
    "                else:\n",
    "                    weight_list.append(1 / dis)\n",
    "            new_weight = softmax(weight_list)\n",
    "            for i in range(len(new_weight)):\n",
    "                if new_weight[i] == 0:\n",
    "                    new_weight[i] = 1\n",
    "            vec = np.zeros(output_size)\n",
    "            for id in range(len(index[index_list_id])):\n",
    "                idx = index[index_list_id][id]\n",
    "                target_list = data_chunk[test_chunk][key_set[idx]][vec_idx]\n",
    "                for ele in target_list:\n",
    "                    vec[ele] += new_weight[id]\n",
    "            outputs.append(vec)\n",
    "        output_vectors.append(outputs)\n",
    "    return output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_history_record1(sum_history, output_size,k):\n",
    "    history_mat = []\n",
    "    for key in sum_history.keys():\n",
    "        history_mat.append(sum_history[key])\n",
    "\n",
    "    print('Finding k nearest neighbors...')\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(history_mat)\n",
    "    distances, indices = nbrs.kneighbors(history_mat)\n",
    "    KNN_history = {}\n",
    "    key_set = list(sum_history)\n",
    "    for id in range(len(key_set)):\n",
    "#    for idx_list in indices:\n",
    "        idx_list = indices[id]\n",
    "        NN_history = np.zeros(output_size)\n",
    "        for idx in idx_list:\n",
    "            NN_history += sum_history[key_set[idx]]\n",
    "        NN_history = NN_history / k\n",
    "        KNN_history[key_set[id]] = NN_history\n",
    "        return KNN_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_history_record2(query_set, sum_history, output_size,k):\n",
    "    history_mat = []\n",
    "    for key in sum_history.keys():\n",
    "        history_mat.append(sum_history[key])\n",
    "    test_mat = []\n",
    "    for key in query_set.keys():\n",
    "        test_mat.append(query_set[key])\n",
    "    print('Finding k nearest neighbors...')\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(history_mat)\n",
    "    distances, indices = nbrs.kneighbors(test_mat)\n",
    "    KNN_history = {}\n",
    "    key_set = list(query_set)\n",
    "    training_key_set = list(sum_history)\n",
    "    for id in range(len(key_set)):\n",
    "#    for idx_list in indices:\n",
    "        idx_list = indices[id]\n",
    "        NN_history = np.zeros(output_size)\n",
    "        for idx in idx_list:\n",
    "            NN_history += sum_history[training_key_set[idx]]\n",
    "        NN_history = NN_history / k\n",
    "        KNN_history[key_set[id]] = NN_history\n",
    "    return KNN_history,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_history_list(his_list,group_size):\n",
    "    grouped_vec_list = []\n",
    "    if len(his_list) < group_size:\n",
    "        #sum = np.zeros(len(his_list[0]))\n",
    "        for j in range(len(his_list)):\n",
    "            grouped_vec_list.append(his_list[j])\n",
    "\n",
    "        return grouped_vec_list, len(his_list)\n",
    "    else:\n",
    "        est_num_vec_each_block = len(his_list)/group_size\n",
    "        base_num_vec_each_block = int(np.floor(len(his_list)/group_size))\n",
    "        residual = est_num_vec_each_block - base_num_vec_each_block\n",
    "\n",
    "        num_vec_has_extra_vec = int(np.round(residual * group_size))\n",
    "\n",
    "        if residual == 0:\n",
    "            for i in range(group_size):\n",
    "                if len(his_list)<1:\n",
    "                    print('len(his_list)<1')\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "        else:\n",
    "\n",
    "            for i in range(group_size - num_vec_has_extra_vec):\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*base_num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                    last_idx = i * base_num_vec_each_block + j\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "\n",
    "            est_num = int(np.ceil(est_num_vec_each_block))\n",
    "            start_group_idx = group_size - num_vec_has_extra_vec\n",
    "            if len(his_list) - start_group_idx*base_num_vec_each_block >= est_num_vec_each_block:\n",
    "                for i in range(start_group_idx,group_size):\n",
    "                    sum = np.zeros(len(his_list[0]))\n",
    "                    for j in range(est_num):\n",
    "                        # if residual+(i-1)*est_num_vec_each_block+j >= len(his_list):\n",
    "                        #     print('residual+(i-1)*num_vec_each_block+j')\n",
    "                        #     print('len(his_list)')\n",
    "                        iidxx = last_idx + 1+(i-start_group_idx)*est_num+j\n",
    "                        if  iidxx >= len(his_list) or iidxx<0:\n",
    "                            print('last_idx + 1+(i-start_group_idx)*est_num+j')\n",
    "                        sum += his_list[iidxx]\n",
    "                    grouped_vec_list.append(sum/est_num)\n",
    "\n",
    "        return grouped_vec_list, group_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_decay_sum_history(data_set, key_set, output_size,group_size,within_decay_rate,group_decay_rate):\n",
    "    sum_history = {}\n",
    "    for key in key_set:\n",
    "        vec_list = data_set[key]\n",
    "        num_vec = len(vec_list) - 2\n",
    "        his_list = []\n",
    "        for idx in range(1,num_vec+1):\n",
    "            his_vec = np.zeros(output_size)\n",
    "            decayed_val = np.power(within_decay_rate,num_vec-idx)\n",
    "            for ele in vec_list[idx]:\n",
    "                his_vec[ele] = decayed_val\n",
    "            his_list.append(his_vec)\n",
    "\n",
    "        grouped_list,real_group_size = group_history_list(his_list,group_size)\n",
    "        his_vec = np.zeros(output_size)\n",
    "        for idx in range(real_group_size):\n",
    "            decayed_val = np.power(group_decay_rate, group_size - 1 - idx)\n",
    "            if idx>=len(grouped_list):\n",
    "                print( 'idx: '+ str(idx))\n",
    "                print('len(grouped_list): ' + str(len(grouped_list)))\n",
    "            his_vec += grouped_list[idx]*decayed_val\n",
    "        sum_history[key] = his_vec/real_group_size\n",
    "        # sum_history[key] = np.multiply(his_vec / real_group_size, IDF)\n",
    "    return sum_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_the_data(data_chunk,key_set):\n",
    "    filtered_key_set = []\n",
    "    for key in key_set:\n",
    "        if len(data_chunk[training_chunk][key])<=3:\n",
    "            continue\n",
    "        if len(data_chunk[test_chunk][key])<2+next_k_step:\n",
    "            continue\n",
    "        filtered_key_set.append(key)\n",
    "\n",
    "    training_key_set = filtered_key_set[0:int(4 / 5 * len(filtered_key_set))]\n",
    "    print(len(training_key_set))\n",
    "    test_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)):]\n",
    "    return training_key_set,test_key_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_the_data_validate(data_chunk, key_set, next_k_step):\n",
    "    filtered_key_set = []\n",
    "    past_chunk = 0\n",
    "    future_chunk = 1\n",
    "    for key in key_set:\n",
    "        if len(data_chunk[past_chunk][key]) <= 3:\n",
    "            continue\n",
    "        if len(data_chunk[future_chunk][key]) < 2 + next_k_step:\n",
    "            continue\n",
    "        filtered_key_set.append(key)\n",
    "\n",
    "    training_key_set = filtered_key_set[0:int(4 / 5 * len(filtered_key_set)*0.9)]\n",
    "    validation_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)*0.9):int(4 / 5 * len(filtered_key_set))]\n",
    "    print('Number of training instances: ' + str(len(training_key_set)))\n",
    "    test_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)):]\n",
    "    return training_key_set, validation_key_set, test_key_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_elements(data_chunk,index,training_key_set,output_size):\n",
    "    output_vectors = []\n",
    "\n",
    "    for vec_idx in range(1,next_k_step+1):\n",
    "        vec = np.zeros(output_size)\n",
    "        for idx in index:\n",
    "            target_vec = data_chunk[test_chunk][training_key_set[idx]][vec_idx]\n",
    "            for ele in target_vec:\n",
    "                vec[ele] += 1\n",
    "\n",
    "        output_vectors.append(vec)\n",
    "    return output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_elements_in_input(sum_history,key):\n",
    "    output_vectors = []\n",
    "\n",
    "    for idx in range(next_k_step):\n",
    "        vec = sum_history[key]\n",
    "        output_vectors.append(vec)\n",
    "    return output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dictionary_BA(files, attributes_list):\n",
    "    path = ''\n",
    "    #files = ['Coborn_history_order.csv','Coborn_future_order.csv']\n",
    "    #files = ['BA_history_order.csv', 'BA_future_order.csv']\n",
    "    #attributes_list = ['MATERIAL_NUMBER']\n",
    "    dictionary_table = {}\n",
    "    counter_table = {}\n",
    "    for attr in attributes_list:\n",
    "        dictionary = {}\n",
    "        dictionary_table[attr] = dictionary\n",
    "        counter_table[attr] = 0\n",
    "\n",
    "    #csv.field_size_limit(sys.maxsize)\n",
    "    for filename in files:\n",
    "        count = 0\n",
    "        with open(path + filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            for row in reader:\n",
    "                if count == 0:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                key = attributes_list[0]\n",
    "                if row[2] not in dictionary_table[key]:\n",
    "                    dictionary_table[key][row[2]] = counter_table[key]\n",
    "                    counter_table[key] = counter_table[key] + 1\n",
    "                    count += 1\n",
    "    print(counter_table)\n",
    "\n",
    "    total = 0\n",
    "    for key in counter_table.keys():\n",
    "        total = total + counter_table[key]\n",
    "\n",
    "    print('# dimensions of final vector: ' + str(total) + ' | '+str(count-1))\n",
    "\n",
    "    return dictionary_table, total, counter_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_claim2vector_embedding_file_no_vector(files):\n",
    "    #attributes_list = ['DRG', 'PROVCAT ', 'RVNU_CD', 'DIAG', 'PROC']\n",
    "    attributes_list = ['MATERIAL_NUMBER']\n",
    "    path = ''\n",
    "    print('start dictionary generation...')\n",
    "    dictionary_table, num_dim, counter_table = generate_dictionary_BA(files, attributes_list)\n",
    "    print('finish dictionary generation*****')\n",
    "    usr_attr = 'CUSTOMER_ID'\n",
    "    ord_attr = 'ORDER_NUMBER'\n",
    "\n",
    "    #dictionary_table, num_dim, counter_table = GDF.generate_dictionary(attributes_list)\n",
    "\n",
    "    freq_max = 200\n",
    "    ## all the follow three ways array. First index is patient, second index is the time step, third is the feature vector\n",
    "    data_chunk = []\n",
    "    day_gap_counter = []\n",
    "    claims_counter = 0\n",
    "    num_claim = 0\n",
    "    code_freq_at_first_claim = np.zeros(num_dim+2)\n",
    "\n",
    "\n",
    "    for file_id in range(len(files)):\n",
    "\n",
    "        count = 0\n",
    "        data_chunk.append({})\n",
    "        filename = files[file_id]\n",
    "        with open(path + filename, 'r') as csvfile:\n",
    "            #gap_within_one_year = np.zeros(365)\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            last_pid_date = '*'\n",
    "            last_pid = '-1'\n",
    "            last_days = -1\n",
    "            # 2 more elements in the end for start and end states\n",
    "            feature_vector = []\n",
    "            for row in reader:\n",
    "                cur_pid_date = row[usr_attr] + '_' + row[ord_attr]\n",
    "                cur_pid = row[usr_attr]\n",
    "                #cur_days = int(row[ord_attr])\n",
    "\n",
    "                if cur_pid != last_pid:\n",
    "                    # start state\n",
    "                    tmp = [-1]\n",
    "                    data_chunk[file_id][cur_pid] = []\n",
    "                    data_chunk[file_id][cur_pid].append(tmp)\n",
    "                    num_claim = 0\n",
    "\n",
    "                if cur_pid_date not in last_pid_date:\n",
    "                    if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                        sorted_feature_vector = np.sort(feature_vector)\n",
    "                        data_chunk[file_id][last_pid].append(sorted_feature_vector)\n",
    "                        if len(sorted_feature_vector) > 0:\n",
    "                            count = count + 1\n",
    "                        #data_chunk[file_id][last_pid].append(feature_vector)\n",
    "                    feature_vector = []\n",
    "\n",
    "                    claims_counter = 0\n",
    "                if cur_pid != last_pid:\n",
    "                    # end state\n",
    "                    if last_pid not in '-1':\n",
    "\n",
    "                        tmp = [-1]\n",
    "                        data_chunk[file_id][last_pid].append(tmp)\n",
    "\n",
    "                key = attributes_list[0]\n",
    "\n",
    "                within_idx = dictionary_table[key][row[key]]\n",
    "                previous_idx = 0\n",
    "\n",
    "                for j in range(attributes_list.index(key)):\n",
    "                    previous_idx = previous_idx + counter_table[attributes_list[j]]\n",
    "                idx = within_idx + previous_idx\n",
    "\n",
    "                # set corresponding dimention to 1\n",
    "                if idx not in feature_vector:\n",
    "                    feature_vector.append(idx)\n",
    "\n",
    "                last_pid_date = cur_pid_date\n",
    "                last_pid = cur_pid\n",
    "                #last_days = cur_days\n",
    "                if file_id == 1:\n",
    "                    claims_counter = claims_counter + 1\n",
    "\n",
    "\n",
    "            if last_pid_date not in '*' and last_pid not in '-1':\n",
    "                data_chunk[file_id][last_pid].append(np.sort(feature_vector))\n",
    "\n",
    "    return data_chunk, num_dim + 2, code_freq_at_first_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall_Fscore(groundtruth,pred):\n",
    "    a = groundtruth\n",
    "    b = pred\n",
    "    correct = 0\n",
    "    truth = 0\n",
    "    positive = 0\n",
    "\n",
    "    for idx in range(len(a)):\n",
    "        if a[idx] == 1:\n",
    "            truth += 1\n",
    "            if b[idx] == 1:\n",
    "                correct += 1\n",
    "        if b[idx] == 1:\n",
    "            positive += 1\n",
    "\n",
    "    flag = 0\n",
    "    if 0 == positive:\n",
    "        precision = 0\n",
    "        flag = 1\n",
    "        #print('postivie is 0')\n",
    "    else:\n",
    "        precision = correct/positive\n",
    "    if 0 == truth:\n",
    "        recall = 0\n",
    "        flag = 1\n",
    "        #print('recall is 0')\n",
    "    else:\n",
    "        recall = correct/truth\n",
    "\n",
    "    if flag == 0 and precision + recall > 0:\n",
    "        F = 2*precision*recall/(precision+recall)\n",
    "    else:\n",
    "        F = 0\n",
    "    return precision, recall, F, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F_score(prediction, test_Y):\n",
    "    jaccard_similarity = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "\n",
    "    count = 0\n",
    "    for idx in range(len(test_Y)):\n",
    "        pred = prediction[idx]\n",
    "        T = 0\n",
    "        P = 0\n",
    "        correct = 0\n",
    "        for id in range(len(pred)):\n",
    "            if test_Y[idx][id] == 1:\n",
    "                T = T + 1\n",
    "                if pred[id] == 1:\n",
    "                    correct = correct + 1\n",
    "            if pred[id] == 1:\n",
    "                P = P + 1\n",
    "\n",
    "        if P == 0 or T == 0:\n",
    "            continue\n",
    "        precision = correct / P\n",
    "        recall = correct / T\n",
    "        prec.append(precision)\n",
    "        rec.append(recall)\n",
    "        if correct == 0:\n",
    "            jaccard_similarity.append(0)\n",
    "        else:\n",
    "            jaccard_similarity.append(2 * precision * recall / (precision + recall))\n",
    "        count = count + 1\n",
    "\n",
    "    print(\n",
    "        'average precision: ' + str(np.mean(prec)))\n",
    "    print('average recall : ' + str(\n",
    "        np.mean(rec)))\n",
    "    print('average F score: ' + str(\n",
    "        np.mean(jaccard_similarity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCG(groundtruth, pred_rank_list,k):\n",
    "    count = 0\n",
    "    dcg = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            dcg += (1)/math.log2(count+1+1)\n",
    "        count += 1\n",
    "\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NDCG1(groundtruth, pred_rank_list,k):\n",
    "    count = 0\n",
    "    dcg = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            dcg += (1)/math.log2(count+1+1)\n",
    "        count += 1\n",
    "    idcg = 0\n",
    "    num_real_item = np.sum(groundtruth)\n",
    "    num_item = int(num_real_item)\n",
    "    for i in range(num_item):\n",
    "        idcg += (1) / math.log2(i + 1 + 1)\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HT(groundtruth, pred_rank_list,k):\n",
    "    count = 0\n",
    "    for pred in pred_rank_list:\n",
    "        if count >= k:\n",
    "            break\n",
    "        if groundtruth[pred] == 1:\n",
    "            return 1\n",
    "        count += 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "#input_size = 100\n",
    "topk = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_history(sum_history_test,test_key_set,training_sum_history_test,training_key_set,index,alpha):\n",
    "    merged_history = {}\n",
    "    for test_key_id in range(len(test_key_set)):\n",
    "        test_key = test_key_set[test_key_id]\n",
    "        test_history = sum_history_test[test_key]\n",
    "        sum_training_history = np.zeros(len(test_history))\n",
    "        for indecis in index[test_key_id]:\n",
    "            training_key = training_key_set[indecis]\n",
    "            sum_training_history += training_sum_history_test[training_key]\n",
    "\n",
    "        sum_training_history = sum_training_history/len(index[test_key_id])\n",
    "\n",
    "        merge = test_history*alpha + sum_training_history*(1-alpha)\n",
    "        merged_history[test_key] = merge\n",
    "\n",
    "    return merged_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_history_and_neighbors_future(future_data, sum_history_test, test_key_set,training_sum_history_test,\n",
    "                                       training_key_set,index,alpha, beta):\n",
    "    merged_history = {}\n",
    "    for test_key_id in range(len(test_key_set)):\n",
    "        test_key = test_key_set[test_key_id]\n",
    "        test_history = sum_history_test[test_key]\n",
    "        sum_training_history = np.zeros(len(test_history))\n",
    "        sum_training_future = np.zeros(len(test_history))\n",
    "        for indecis in index[test_key_id]:\n",
    "            training_key = training_key_set[indecis]\n",
    "            sum_training_history += training_sum_history_test[training_key]\n",
    "            # future_vec = np.zeros((len(test_history)))\n",
    "            for idx in future_data[training_key][1]:\n",
    "                if idx >= 0:\n",
    "                    sum_training_future[idx] += 1\n",
    "\n",
    "        sum_training_history = sum_training_history/len(index[test_key_id])\n",
    "        sum_training_future = sum_training_future/len(index[test_key_id])\n",
    "\n",
    "        merge = (test_history*alpha + sum_training_history*(1-alpha))* beta + sum_training_future*(1-beta)\n",
    "        merged_history[test_key] = merge\n",
    "\n",
    "    return merged_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_chunk,  training_key_set, test_key_set, input_size, group_size,\n",
    "             within_decay_rate, group_decay_rate, num_nearest_neighbors, alpha,  topk):\n",
    "    activate_codes_num = -1\n",
    "    temporal_decay_sum_history_training = temporal_decay_sum_history(data_chunk[training_chunk],\n",
    "                                                                     training_key_set, input_size,\n",
    "                                                                     group_size, within_decay_rate,\n",
    "                                                                     group_decay_rate)\n",
    "    temporal_decay_sum_history_test = temporal_decay_sum_history(data_chunk[training_chunk],\n",
    "                                                                 test_key_set, input_size,\n",
    "                                                                 group_size, within_decay_rate,\n",
    "                                                                 group_decay_rate)\n",
    "    index, distance = KNN(temporal_decay_sum_history_test, temporal_decay_sum_history_training,\n",
    "                          num_nearest_neighbors)\n",
    "\n",
    "\n",
    "    sum_history = merge_history(temporal_decay_sum_history_test, test_key_set, temporal_decay_sum_history_training,\n",
    "                                training_key_set, index, alpha)\n",
    "\n",
    "\n",
    "    if activate_codes_num < 0:\n",
    "        # for i in range(1, 6):\n",
    "\n",
    "        prec = []\n",
    "        rec = []\n",
    "        F = []\n",
    "        prec1 = []\n",
    "        rec1 = []\n",
    "        F1 = []\n",
    "        prec2 = []\n",
    "        rec2 = []\n",
    "        F2 = []\n",
    "        prec3 = []\n",
    "        rec3 = []\n",
    "        F3 = []\n",
    "        NDCG = []\n",
    "        n_hit = 0\n",
    "\n",
    "        num_ele = topk\n",
    "        # print('k = ' + str(activate_codes_num))\n",
    "        # evaluate(data_chunk, input_size,test_KNN_history, test_key_set, next_k_step)\n",
    "        count = 0\n",
    "        for iter in range(len(test_key_set)):\n",
    "            # training_pair = training_pairs[iter - 1]\n",
    "            # input_variable = training_pair[0]\n",
    "            # target_variable = training_pair[1]\n",
    "            input_variable = data_chunk[training_chunk][test_key_set[iter]]\n",
    "            target_variable = data_chunk[test_chunk][test_key_set[iter]]\n",
    "\n",
    "            if len(target_variable) < 2 + next_k_step:\n",
    "                continue\n",
    "            count += 1\n",
    "            output_vectors = predict_with_elements_in_input(sum_history, test_key_set[iter])\n",
    "            top = 400\n",
    "            hit = 0\n",
    "            for idx in range(len(output_vectors)):\n",
    "                # for idx in [2]:\n",
    "\n",
    "                output = np.zeros(input_size)\n",
    "                target_topi = output_vectors[idx].argsort()[::-1][:top]\n",
    "                c = 0\n",
    "                for i in range(top):\n",
    "                    if c >= num_ele:\n",
    "                        break\n",
    "                    output[target_topi[i]] = 1\n",
    "                    c += 1\n",
    "\n",
    "                vectorized_target = np.zeros(input_size)\n",
    "                for ii in target_variable[1 + idx]:\n",
    "                    vectorized_target[ii] = 1\n",
    "                precision, recall, Fscore, correct = get_precision_recall_Fscore \\\n",
    "                    (vectorized_target, output)\n",
    "                prec.append(precision)\n",
    "                rec.append(recall)\n",
    "                F.append(Fscore)\n",
    "                if idx == 0:\n",
    "                    prec1.append(precision)\n",
    "                    rec1.append(recall)\n",
    "                    F1.append(Fscore)\n",
    "                elif idx == 1:\n",
    "                    prec2.append(precision)\n",
    "                    rec2.append(recall)\n",
    "                    F2.append(Fscore)\n",
    "                elif idx == 2:\n",
    "                    prec3.append(precision)\n",
    "                    rec3.append(recall)\n",
    "                    F3.append(Fscore)\n",
    "                hit += get_HT(vectorized_target, target_topi, num_ele)\n",
    "                ndcg = get_NDCG1(vectorized_target, target_topi, num_ele)\n",
    "                NDCG.append(ndcg)\n",
    "            if hit == next_k_step:\n",
    "                n_hit += 1\n",
    "\n",
    "\n",
    "        # print('average precision of ' + ': ' + str(np.mean(prec)) + ' with std: ' + str(np.std(prec)))\n",
    "        recall = np.mean(rec)\n",
    "        ndcg = np.mean(NDCG)\n",
    "        hr = n_hit / len(test_key_set)\n",
    "\n",
    "    return recall, ndcg, hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nearest_neighbors = 300\n",
    "within_decay_rate = 0.9\n",
    "group_decay_rate = 0.7\n",
    "alpha = 0.7\n",
    "group_size = 7\n",
    "topk = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['TaFang_history_NB.csv', 'TaFang_future_NB.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start dictionary generation...\n",
      "{'MATERIAL_NUMBER': 11997}\n",
      "# dimensions of final vector: 11997 | 0\n",
      "finish dictionary generation*****\n"
     ]
    }
   ],
   "source": [
    "data_chunk, input_size, code_freq_at_first_claim = read_claim2vector_embedding_file_no_vector(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 10000\n"
     ]
    }
   ],
   "source": [
    "training_key_set, validation_key_set, test_key_set = partition_the_data_validate(data_chunk, list(data_chunk[test_chunk]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of top:  10\n",
      "recall:  0.12933969065750625\n",
      "NDCG:  0.09859257577070348\n"
     ]
    }
   ],
   "source": [
    "print('Num. of top: ', topk)\n",
    "recall, ndcg, hr = evaluate(data_chunk, training_key_set, test_key_set, input_size,\n",
    "                            group_size, within_decay_rate, group_decay_rate,\n",
    "                            num_nearest_neighbors, alpha,  topk)\n",
    "\n",
    "print('recall: ', str(recall))\n",
    "print('NDCG: ', str(ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "def main(argv):\n",
    "\n",
    "\n",
    "    files = [argv[1], argv[2]]\n",
    "\n",
    "    data_chunk, input_size, code_freq_at_first_claim = read_claim2vector_embedding_file_no_vector(files)\n",
    "\n",
    "    training_key_set, validation_key_set, test_key_set = partition_the_data_validate(data_chunk, list(data_chunk[test_chunk]), 1)\n",
    "\n",
    "\n",
    "\n",
    "    num_nearest_neighbors = int(argv[3])\n",
    "    within_decay_rate = float(argv[4])\n",
    "    group_decay_rate = float(argv[5])\n",
    "    alpha = float(argv[6])\n",
    "    group_size = int(argv[7])\n",
    "    topk = int(argv[8])\n",
    "\n",
    "    # num_nearest_neighbors = 300\n",
    "    # within_decay_rate = 0.9\n",
    "    # group_decay_rate = 0.7\n",
    "    # alpha = 0.7\n",
    "    # group_size = 7\n",
    "    # topk = 10\n",
    "\n",
    "\n",
    "    print('Num. of top: ', topk)\n",
    "    recall, ndcg, hr = evaluate(data_chunk, training_key_set, test_key_set, input_size,\n",
    "                                group_size, within_decay_rate, group_decay_rate,\n",
    "                                num_nearest_neighbors, alpha,  topk)\n",
    "\n",
    "    print('recall: ', str(recall))\n",
    "    print('NDCG: ', str(ndcg))\n",
    "    # print('hit ratio: ', str(hr))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
