{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nearest_neighbors = 300#k (num_nearest_neighbors) chosen [100, 300, 500, 700, 900, 1100,1300]\n",
    "within_decay_rate = 0.9# chosen from [0.1, 0.2, 0.3,0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "group_decay_rate = 0.7# chosen from [0.1, 0.2, 0.3,0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "alpha = 0.7\n",
    "group_size = 7\n",
    "topk = 10\n",
    "\n",
    "#files = ['history.csv', 'future.csv']\n",
    "\n",
    "files = ['TaFang_history_NB.csv', 'TaFang_future_NB.csv']\n",
    "attributes_list = 'MATERIAL_NUMBER'\n",
    "usr_attr = 'CUSTOMER_ID'\n",
    "ord_attr = 'ORDER_NUMBER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 10001\n",
      "Num. of top:  10\n",
      "Finding k nearest neighbors...\n",
      "Finish KNN search.\n",
      "average precision of : 0.053364519611371 with std: 0.07979832834575636\n",
      "recall:  0.1296420382929529\n",
      "NDCG:  0.09875611529474698\n"
     ]
    }
   ],
   "source": [
    "next_k_step = 1\n",
    "freq_max = 200\n",
    "## all the follow three ways array. First index is patient, second index is the time step, third is the feature vector\n",
    "data_chunk = []\n",
    "day_gap_counter = []\n",
    "claims_counter = 0\n",
    "num_claim = 0\n",
    "activate_codes_num = -1\n",
    "training_chunk = 0\n",
    "test_chunk = 1\n",
    "\n",
    "df1=pd.read_csv(files[0])\n",
    "tmp1=df1.sort_values([usr_attr,ord_attr]).groupby([usr_attr,ord_attr])[attributes_list].apply(np.array).reset_index()\n",
    "tmp1=tmp1.groupby([usr_attr])[attributes_list].apply(list).reset_index()\n",
    "tmp1[attributes_list]=tmp1[attributes_list].apply(lambda x: [[-1]]+[np.array(sorted(i)) for i in x]+[[-1]])\n",
    "\n",
    "df2=pd.read_csv(files[1])\n",
    "tmp2=df2.sort_values([usr_attr,ord_attr]).groupby([usr_attr,ord_attr])[attributes_list].apply(np.array).reset_index()\n",
    "tmp2=tmp2.groupby([usr_attr])[attributes_list].apply(list).reset_index()\n",
    "tmp2[attributes_list]=tmp2[attributes_list].apply(lambda x: [[-1]]+[np.array(sorted(i)) for i in x]+[[-1]])\n",
    "\n",
    "past_chunk = 0\n",
    "future_chunk = 1\n",
    "\n",
    "history_key_set,future_key_set=tmp1,tmp2\n",
    "\n",
    "history_key_set[attributes_list]=np.where(tmp1[attributes_list].apply(len)<= 3,None,tmp1[attributes_list])\n",
    "history_key_set[usr_attr]=history_key_set[usr_attr].apply(str)\n",
    "history_key_set=list(history_key_set[~history_key_set[attributes_list].isnull()][usr_attr])\n",
    "\n",
    "future_key_set[attributes_list]=np.where(tmp2[attributes_list].apply(len)< 2 + next_k_step,None,tmp2[attributes_list])\n",
    "future_key_set[usr_attr]=future_key_set[usr_attr].apply(str)\n",
    "future_key_set=list(future_key_set[~future_key_set[attributes_list].isnull()][usr_attr])\n",
    "\n",
    "filtered_key_set=list(pd.Series(history_key_set)[pd.Series(history_key_set).isin(future_key_set) & pd.Series(history_key_set).isin(future_key_set).isin(history_key_set)])\n",
    "#filtered_key_set=[x for x in history_key_set if x in future_key_set]\n",
    "#slice filtered_key_set range(0:10001)\n",
    "training_key_set = filtered_key_set[0:int(4 / 5 * len(filtered_key_set)*0.9)]\n",
    "print('Number of training instances: ' + str(len(training_key_set)))\n",
    "#slice filtered_key_set range(10001:11112)\n",
    "validation_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)*0.9):int(4 / 5 * len(filtered_key_set))]\n",
    "#slice filtered_key_set range(:11112)\n",
    "test_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)):]\n",
    "\n",
    "num_dim=len(set(df1[attributes_list]))\n",
    "code_freq_at_first_claim = np.zeros(num_dim+2)\n",
    "data_chunk=[dict(zip(tmp1[usr_attr].apply(str),tmp1[attributes_list])),dict(zip(tmp2[usr_attr].apply(str),tmp2[attributes_list]))]\n",
    "\n",
    "#data_chunk=data_chunk\n",
    "input_size=num_dim + 2\n",
    "#code_freq_at_first_claim=code_freq_at_first_claim \n",
    "#data_chunk=data_chunk\n",
    "key_set=list(data_chunk[test_chunk])\n",
    "data_set=data_chunk[training_chunk]\n",
    "key_set=training_key_set \n",
    "output_size=input_size\n",
    "#within_decay_rate=within_decay_rate\n",
    "#group_decay_rate=group_decay_rate\n",
    "print('Num. of top: ', topk)\n",
    "\n",
    "sum_history = {}\n",
    "for key in key_set:\n",
    "    vec_list = data_set[key]\n",
    "    num_vec = len(vec_list)-2\n",
    "    his_list = []\n",
    "    \n",
    "    for idx in range(1,num_vec+1):\n",
    "        his_vec = np.zeros(output_size)#output_size=11999\n",
    "        # np.power(.9,15-iterater)  #counts down \n",
    "        decayed_val = np.power(within_decay_rate,num_vec-idx)\n",
    "        #elements in data_set['2']\n",
    "        for ele in vec_list[idx]:\n",
    "            his_vec[ele] = decayed_val\n",
    "        his_list.append(his_vec)\n",
    "        \n",
    "    grouped_vec_list = []\n",
    "    if len(his_list) < group_size:\n",
    "        #sum = np.zeros(len(his_list[0]))\n",
    "        for j in range(len(his_list)):\n",
    "            grouped_vec_list.append(his_list[j])\n",
    "\n",
    "        grouped_list,real_group_size = grouped_vec_list, len(his_list)\n",
    "    else:\n",
    "        est_num_vec_each_block = len(his_list)/group_size\n",
    "        base_num_vec_each_block = int(np.floor(len(his_list)/group_size))\n",
    "        residual = est_num_vec_each_block - base_num_vec_each_block\n",
    "\n",
    "        num_vec_has_extra_vec = int(np.round(residual * group_size))\n",
    "\n",
    "        if residual == 0:\n",
    "            for i in range(group_size):\n",
    "                if len(his_list)<1:\n",
    "                    print('len(his_list)<1')\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "        else:\n",
    "            for i in range(group_size - num_vec_has_extra_vec):\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*base_num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                    last_idx = i * base_num_vec_each_block + j\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "\n",
    "            est_num = int(np.ceil(est_num_vec_each_block))\n",
    "            start_group_idx = group_size - num_vec_has_extra_vec\n",
    "            if len(his_list) - start_group_idx*base_num_vec_each_block >= est_num_vec_each_block:\n",
    "                for i in range(start_group_idx,group_size):\n",
    "                    sum = np.zeros(len(his_list[0]))\n",
    "                    for j in range(est_num):\n",
    "                        # if residual+(i-1)*est_num_vec_each_block+j >= len(his_list):\n",
    "                        #     print('residual+(i-1)*num_vec_each_block+j')\n",
    "                        #     print('len(his_list)')\n",
    "                        iidxx = last_idx + 1+(i-start_group_idx)*est_num+j\n",
    "                        if  iidxx >= len(his_list) or iidxx<0:\n",
    "                            print('last_idx + 1+(i-start_group_idx)*est_num+j')\n",
    "                        sum += his_list[iidxx]\n",
    "                    grouped_vec_list.append(sum/est_num)\n",
    "\n",
    "        grouped_list,real_group_size = grouped_vec_list, group_size\n",
    "\n",
    "    his_vec = np.zeros(output_size)\n",
    "    for idx in range(real_group_size):\n",
    "        decayed_val = np.power(group_decay_rate, group_size - 1 - idx)\n",
    "        if idx>=len(grouped_list):\n",
    "            print( 'idx: '+ str(idx))\n",
    "            print('len(grouped_list): ' + str(len(grouped_list)))\n",
    "        his_vec += grouped_list[idx]*decayed_val\n",
    "    sum_history[key] = his_vec/real_group_size\n",
    "    # sum_history[key] = np.multiply(his_vec / real_group_size, IDF)\n",
    "\n",
    "temporal_decay_sum_history_training = sum_history\n",
    "\n",
    "data_set=data_chunk[training_chunk] \n",
    "key_set=test_key_set \n",
    "output_size=input_size\n",
    "#group_size=group_size\n",
    "#within_decay_rate=within_decay_rate\n",
    "#group_decay_rate=group_decay_rate\n",
    "#data_set, key_set, output_size,group_size,within_decay_rate,group_decay_rate=data_chunk[training_chunk],test_key_set, input_size,group_size, within_decay_rate,group_decay_rate\n",
    "\n",
    "sum_history = {}\n",
    "for key in key_set:\n",
    "    vec_list = data_set[key]\n",
    "    num_vec = len(vec_list) - 2\n",
    "    his_list = []\n",
    "    \n",
    "        #removes the [-1] flags in data\n",
    "    for idx in range(1,num_vec+1):\n",
    "        his_vec = np.zeros(output_size)#output_size=11999\n",
    "        # np.power(.9,15-iterater)  #counts down \n",
    "        decayed_val = np.power(within_decay_rate,num_vec-idx)\n",
    "        #elements in data_set['2']\n",
    "        for ele in vec_list[idx]:\n",
    "            his_vec[ele] = decayed_val\n",
    "        his_list.append(his_vec)\n",
    "        \n",
    "    grouped_vec_list = []\n",
    "    if len(his_list) < group_size:\n",
    "        #sum = np.zeros(len(his_list[0]))\n",
    "        for j in range(len(his_list)):\n",
    "            grouped_vec_list.append(his_list[j])\n",
    "\n",
    "        grouped_list,real_group_size = grouped_vec_list, len(his_list)\n",
    "    else:\n",
    "        est_num_vec_each_block = len(his_list)/group_size\n",
    "        base_num_vec_each_block = int(np.floor(len(his_list)/group_size))\n",
    "        residual = est_num_vec_each_block - base_num_vec_each_block\n",
    "\n",
    "        num_vec_has_extra_vec = int(np.round(residual * group_size))\n",
    "\n",
    "        if residual == 0:\n",
    "            for i in range(group_size):\n",
    "                if len(his_list)<1:\n",
    "                    print('len(his_list)<1')\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "        else:\n",
    "            for i in range(group_size - num_vec_has_extra_vec):\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*base_num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                    last_idx = i * base_num_vec_each_block + j\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "\n",
    "            est_num = int(np.ceil(est_num_vec_each_block))\n",
    "            start_group_idx = group_size - num_vec_has_extra_vec\n",
    "            if len(his_list) - start_group_idx*base_num_vec_each_block >= est_num_vec_each_block:\n",
    "                for i in range(start_group_idx,group_size):\n",
    "                    sum = np.zeros(len(his_list[0]))\n",
    "                    for j in range(est_num):\n",
    "                        # if residual+(i-1)*est_num_vec_each_block+j >= len(his_list):\n",
    "                        #     print('residual+(i-1)*num_vec_each_block+j')\n",
    "                        #     print('len(his_list)')\n",
    "                        iidxx = last_idx + 1+(i-start_group_idx)*est_num+j\n",
    "                        if  iidxx >= len(his_list) or iidxx<0:\n",
    "                            print('last_idx + 1+(i-start_group_idx)*est_num+j')\n",
    "                        sum += his_list[iidxx]\n",
    "                    grouped_vec_list.append(sum/est_num)\n",
    "\n",
    "        grouped_list,real_group_size = grouped_vec_list, group_size\n",
    "    his_vec = np.zeros(output_size)\n",
    "    for idx in range(real_group_size):\n",
    "        decayed_val = np.power(group_decay_rate, group_size - 1 - idx)\n",
    "        if idx>=len(grouped_list):\n",
    "            print( 'idx: '+ str(idx))\n",
    "            print('len(grouped_list): ' + str(len(grouped_list)))\n",
    "        his_vec += grouped_list[idx]*decayed_val\n",
    "    sum_history[key] = his_vec/real_group_size\n",
    "    # sum_history[key] = np.multiply(his_vec / real_group_size, IDF)\n",
    "    \n",
    "temporal_decay_sum_history_test = sum_history\n",
    "query_set=temporal_decay_sum_history_test \n",
    "target_set=temporal_decay_sum_history_training \n",
    "k=num_nearest_neighbors\n",
    "\n",
    "#def KNN(query_set, target_set, k):\n",
    "history_mat = []\n",
    "for key in target_set.keys():\n",
    "    history_mat.append(target_set[key])\n",
    "test_mat = []\n",
    "for key in query_set.keys():\n",
    "    test_mat.append(query_set[key])\n",
    "print('Finding k nearest neighbors...')\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(history_mat)\n",
    "distances, indices = nbrs.kneighbors(test_mat)\n",
    "print('Finish KNN search.' )\n",
    "index, distance = indices,distances\n",
    "\n",
    "sum_history_test,test_key_set,training_sum_history_test,training_key_set,index,alpha = temporal_decay_sum_history_test, test_key_set, temporal_decay_sum_history_training,training_key_set, index, alpha\n",
    "merged_history = {}\n",
    "for test_key_id in range(len(test_key_set)):\n",
    "    test_key = test_key_set[test_key_id]\n",
    "    test_history = sum_history_test[test_key]\n",
    "    sum_training_history = np.zeros(len(test_history))\n",
    "    for indecis in index[test_key_id]:\n",
    "        training_key = training_key_set[indecis]\n",
    "        sum_training_history += training_sum_history_test[training_key]\n",
    "\n",
    "    sum_training_history = sum_training_history/len(index[test_key_id])\n",
    "\n",
    "    merge = test_history*alpha + sum_training_history*(1-alpha)\n",
    "    merged_history[test_key] = merge\n",
    "\n",
    "sum_history = merged_history\n",
    "\n",
    "if activate_codes_num < 0:\n",
    "    # for i in range(1, 6):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    F = []\n",
    "    prec1 = []\n",
    "    rec1 = []\n",
    "    F1 = []\n",
    "    prec2 = []\n",
    "    rec2 = []\n",
    "    F2 = []\n",
    "    prec3 = []\n",
    "    rec3 = []\n",
    "    F3 = []\n",
    "    NDCG = []\n",
    "    n_hit = 0\n",
    "\n",
    "    num_ele = topk\n",
    "    # print('k = ' + str(activate_codes_num))\n",
    "    # evaluate(data_chunk, input_size,test_KNN_history, test_key_set, next_k_step)\n",
    "    count = 0\n",
    "    for iter in range(len(test_key_set)):\n",
    "        \n",
    "        # training_pair = training_pairs[iter - 1]\n",
    "        # input_variable = training_pair[0]\n",
    "        # target_variable = training_pair[1]\n",
    "        input_variable = data_chunk[training_chunk][test_key_set[iter]]\n",
    "        target_variable = data_chunk[test_chunk][test_key_set[iter]]\n",
    "\n",
    "        if len(target_variable) < 2 + next_k_step:\n",
    "            continue\n",
    "        count += 1\n",
    "        sum_history,key = sum_history, test_key_set[iter]\n",
    "        #def predict_with_elements_in_input(sum_history,key):\n",
    "        output_vectors = []\n",
    "\n",
    "        for idx in range(next_k_step):\n",
    "            vec = sum_history[key]\n",
    "            output_vectors.append(vec)\n",
    "        output_vectors = output_vectors        \n",
    "        top = 400\n",
    "        hit = 0\n",
    "        for idx in range(len(output_vectors)):\n",
    "            # for idx in [2]:\n",
    "\n",
    "            output = np.zeros(input_size)\n",
    "            target_topi = output_vectors[idx].argsort()[::-1][:top]\n",
    "            c = 0\n",
    "            for i in range(top):\n",
    "                if c >= num_ele:\n",
    "                    break\n",
    "                output[target_topi[i]] = 1\n",
    "                c += 1\n",
    "\n",
    "            vectorized_target = np.zeros(input_size)\n",
    "            for ii in target_variable[1 + idx]:\n",
    "                vectorized_target[ii] = 1\n",
    "            groundtruth,pred = vectorized_target, output\n",
    "            #def get_precision_recall_Fscore(groundtruth,pred):\n",
    "            a = groundtruth\n",
    "            b = pred\n",
    "            correct = 0\n",
    "            truth = 0\n",
    "            positive = 0\n",
    "\n",
    "            for idx in range(len(a)):\n",
    "                if a[idx] == 1:\n",
    "                    truth += 1\n",
    "                    if b[idx] == 1:\n",
    "                        correct += 1\n",
    "                if b[idx] == 1:\n",
    "                    positive += 1\n",
    "\n",
    "            flag = 0\n",
    "            if 0 == positive:\n",
    "                precision = 0\n",
    "                flag = 1\n",
    "                #print('postivie is 0')\n",
    "            else:\n",
    "                precision = correct/positive\n",
    "            if 0 == truth:\n",
    "                recall = 0\n",
    "                flag = 1\n",
    "                #print('recall is 0')\n",
    "            else:\n",
    "                recall = correct/truth\n",
    "\n",
    "            if flag == 0 and precision + recall > 0:\n",
    "                G = 2*precision*recall/(precision+recall)\n",
    "            else:\n",
    "                G = 0\n",
    "            precision, recall, Fscore, correct = precision, recall, G, correct            \n",
    "            prec.append(precision)\n",
    "            rec.append(recall)\n",
    "            F.append(Fscore)\n",
    "            if idx == 0:\n",
    "                prec1.append(precision)\n",
    "                rec1.append(recall)\n",
    "                F1.append(Fscore)\n",
    "            elif idx == 1:\n",
    "                prec2.append(precision)\n",
    "                rec2.append(recall)\n",
    "                F2.append(Fscore)\n",
    "            elif idx == 2:\n",
    "                prec3.append(precision)\n",
    "                rec3.append(recall)\n",
    "                F3.append(Fscore)\n",
    "            groundtruth, pred_rank_list,k=vectorized_target, target_topi, num_ele\n",
    "            #def get_HT(groundtruth, pred_rank_list,k):\n",
    "            count = 0\n",
    "            for pred in pred_rank_list:\n",
    "                if count >= k:\n",
    "                    break\n",
    "                if groundtruth[pred] == 1:\n",
    "                    hit += 1\n",
    "                count += 1\n",
    "            hit += 0\n",
    "            #input_size = 100\n",
    "            vectorized_target, target_topi, num_ele=groundtruth, pred_rank_list,k\n",
    "            #def get_NDCG1(groundtruth, pred_rank_list,k):\n",
    "            count = 0\n",
    "            dcg = 0\n",
    "            for pred in pred_rank_list:\n",
    "                if count >= k:\n",
    "                    break\n",
    "                if groundtruth[pred] == 1:\n",
    "                    dcg += (1)/math.log2(count+1+1)\n",
    "                count += 1\n",
    "            idcg = 0\n",
    "            num_real_item = np.sum(groundtruth)\n",
    "            num_item = int(num_real_item)\n",
    "            for i in range(num_item):\n",
    "                idcg += (1) / math.log2(i + 1 + 1)\n",
    "            ndcg = dcg / idcg\n",
    "            NDCG.append(ndcg)\n",
    "        if hit == next_k_step:\n",
    "            n_hit += 1\n",
    "            \n",
    "    print('average precision of ' + ': ' + str(np.mean(prec)) + ' with std: ' + str(np.std(prec)))\n",
    "    recall = np.mean(rec)\n",
    "    ndcg = np.mean(NDCG)\n",
    "    hr = n_hit / len(test_key_set)\n",
    "    \n",
    "recall, ndcg, hr = recall, ndcg, hr\n",
    "print('recall: ', str(recall))\n",
    "print('NDCG: ', str(ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import csv\n",
    "\n",
    "sys.maxsize\n",
    "sys.maxsize\n",
    "\n",
    "num_nearest_neighbors = 300#k (num_nearest_neighbors) chosen [100, 300, 500, 700, 900, 1100,1300]\n",
    "within_decay_rate = 0.9# chosen from [0.1, 0.2, 0.3,0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "group_decay_rate = 0.7# chosen from [0.1, 0.2, 0.3,0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "alpha = 0.7\n",
    "group_size = 7\n",
    "topk = 10\n",
    "\n",
    "next_k_step = 1\n",
    "freq_max = 200\n",
    "## all the follow three ways array. First index is patient, second index is the time step, third is the feature vector\n",
    "data_chunk = []\n",
    "day_gap_counter = []\n",
    "claims_counter = 0\n",
    "num_claim = 0\n",
    "activate_codes_num = -1\n",
    "training_chunk = 0\n",
    "test_chunk = 1\n",
    "\n",
    "files = ['TaFang_history_NB.csv', 'TaFang_future_NB.csv']\n",
    "attributes_list = 'MATERIAL_NUMBER'\n",
    "usr_attr = 'CUSTOMER_ID'\n",
    "ord_attr = 'ORDER_NUMBER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(files[0])\n",
    "tmp1=df1.sort_values([usr_attr,ord_attr]).groupby([usr_attr,ord_attr])[attributes_list].apply(np.array).reset_index()\n",
    "tmp1=tmp1.groupby([usr_attr])[attributes_list].apply(list).reset_index()\n",
    "tmp1[attributes_list]=tmp1[attributes_list].apply(lambda x: [[-1]]+[np.array(sorted(i)) for i in x]+[[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv(files[1])\n",
    "tmp2=df2.sort_values([usr_attr,ord_attr]).groupby([usr_attr,ord_attr])[attributes_list].apply(np.array).reset_index()\n",
    "tmp2=tmp2.groupby([usr_attr])[attributes_list].apply(list).reset_index()\n",
    "tmp2[attributes_list]=tmp2[attributes_list].apply(lambda x: [[-1]]+[np.array(sorted(i)) for i in x]+[[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 10001\n"
     ]
    }
   ],
   "source": [
    "past_chunk = 0\n",
    "future_chunk = 1\n",
    "history_key_set,future_key_set=tmp1,tmp2\n",
    "\n",
    "history_key_set[attributes_list]=np.where(tmp1[attributes_list].apply(len)<= 3,None,tmp1[attributes_list])\n",
    "history_key_set[usr_attr]=history_key_set[usr_attr].apply(str)\n",
    "history_key_set=list(history_key_set[~history_key_set[attributes_list].isnull()][usr_attr])\n",
    "\n",
    "future_key_set[attributes_list]=np.where(tmp2[attributes_list].apply(len)< 2 + next_k_step,None,tmp2[attributes_list])\n",
    "future_key_set[usr_attr]=future_key_set[usr_attr].apply(str)\n",
    "future_key_set=list(future_key_set[~future_key_set[attributes_list].isnull()][usr_attr])\n",
    "\n",
    "filtered_key_set=list(pd.Series(history_key_set)[pd.Series(history_key_set).isin(future_key_set) & pd.Series(history_key_set).isin(future_key_set).isin(history_key_set)])\n",
    "#filtered_key_set=[x for x in history_key_set if x in future_key_set]\n",
    "#slice filtered_key_set range(0:10001)\n",
    "training_key_set = filtered_key_set[0:int(4 / 5 * len(filtered_key_set)*0.9)]\n",
    "print('Number of training instances: ' + str(len(training_key_set)))\n",
    "#slice filtered_key_set range(10001:11112)\n",
    "validation_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)*0.9):int(4 / 5 * len(filtered_key_set))]\n",
    "#slice filtered_key_set range(:11112)\n",
    "test_key_set = filtered_key_set[int(4 / 5 * len(filtered_key_set)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim=len(set(df1[attributes_list]))\n",
    "code_freq_at_first_claim = np.zeros(num_dim+2)\n",
    "\n",
    "data_chunk=[dict(zip(tmp1[usr_attr].apply(str),tmp1[attributes_list])),dict(zip(tmp2[usr_attr].apply(str),tmp2[attributes_list]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_chunk=data_chunk\n",
    "input_size=num_dim + 2\n",
    "#code_freq_at_first_claim=code_freq_at_first_claim \n",
    "#data_chunk=data_chunk\n",
    "key_set=list(data_chunk[test_chunk]),\n",
    "next_k_step=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of top:  10\n"
     ]
    }
   ],
   "source": [
    "data_set=data_chunk[training_chunk]\n",
    "key_set=training_key_set \n",
    "output_size=input_size\n",
    "#within_decay_rate=within_decay_rate\n",
    "#group_decay_rate=group_decay_rate\n",
    "print('Num. of top: ', topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_history = {}\n",
    "for key in key_set:\n",
    "    vec_list = data_set[key]\n",
    "    num_vec = len(vec_list)-2\n",
    "    his_list = []\n",
    "    \n",
    "    for idx in range(1,num_vec+1):\n",
    "        his_vec = np.zeros(output_size)#output_size=11999\n",
    "        # np.power(.9,15-iterater)  #counts down \n",
    "        decayed_val = np.power(within_decay_rate,num_vec-idx)\n",
    "        #elements in data_set['2']\n",
    "        for ele in vec_list[idx]:\n",
    "            his_vec[ele] = decayed_val\n",
    "        his_list.append(his_vec)\n",
    "        \n",
    "    grouped_vec_list = []\n",
    "    if len(his_list) < group_size:\n",
    "        #sum = np.zeros(len(his_list[0]))\n",
    "        for j in range(len(his_list)):\n",
    "            grouped_vec_list.append(his_list[j])\n",
    "\n",
    "        grouped_list,real_group_size = grouped_vec_list, len(his_list)\n",
    "    else:\n",
    "        est_num_vec_each_block = len(his_list)/group_size\n",
    "        base_num_vec_each_block = int(np.floor(len(his_list)/group_size))\n",
    "        residual = est_num_vec_each_block - base_num_vec_each_block\n",
    "\n",
    "        num_vec_has_extra_vec = int(np.round(residual * group_size))\n",
    "\n",
    "        if residual == 0:\n",
    "            for i in range(group_size):\n",
    "                if len(his_list)<1:\n",
    "                    print('len(his_list)<1')\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "        else:\n",
    "            for i in range(group_size - num_vec_has_extra_vec):\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*base_num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                    last_idx = i * base_num_vec_each_block + j\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "\n",
    "            est_num = int(np.ceil(est_num_vec_each_block))\n",
    "            start_group_idx = group_size - num_vec_has_extra_vec\n",
    "            if len(his_list) - start_group_idx*base_num_vec_each_block >= est_num_vec_each_block:\n",
    "                for i in range(start_group_idx,group_size):\n",
    "                    sum = np.zeros(len(his_list[0]))\n",
    "                    for j in range(est_num):\n",
    "                        # if residual+(i-1)*est_num_vec_each_block+j >= len(his_list):\n",
    "                        #     print('residual+(i-1)*num_vec_each_block+j')\n",
    "                        #     print('len(his_list)')\n",
    "                        iidxx = last_idx + 1+(i-start_group_idx)*est_num+j\n",
    "                        if  iidxx >= len(his_list) or iidxx<0:\n",
    "                            print('last_idx + 1+(i-start_group_idx)*est_num+j')\n",
    "                        sum += his_list[iidxx]\n",
    "                    grouped_vec_list.append(sum/est_num)\n",
    "\n",
    "        grouped_list,real_group_size = grouped_vec_list, group_size\n",
    "\n",
    "    his_vec = np.zeros(output_size)\n",
    "    for idx in range(real_group_size):\n",
    "        decayed_val = np.power(group_decay_rate, group_size - 1 - idx)\n",
    "        if idx>=len(grouped_list):\n",
    "            print( 'idx: '+ str(idx))\n",
    "            print('len(grouped_list): ' + str(len(grouped_list)))\n",
    "        his_vec += grouped_list[idx]*decayed_val\n",
    "    sum_history[key] = his_vec/real_group_size\n",
    "    # sum_history[key] = np.multiply(his_vec / real_group_size, IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_decay_sum_history_training = sum_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=data_chunk[training_chunk] \n",
    "key_set=test_key_set \n",
    "output_size=input_size\n",
    "group_size=group_size\n",
    "within_decay_rate=within_decay_rate\n",
    "group_decay_rate=group_decay_rate\n",
    "#data_set, key_set, output_size,group_size,within_decay_rate,group_decay_rate=data_chunk[training_chunk],test_key_set, input_size,group_size, within_decay_rate,group_decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_history = {}\n",
    "for key in key_set:\n",
    "    vec_list = data_set[key]\n",
    "    num_vec = len(vec_list) - 2\n",
    "    his_list = []\n",
    "    \n",
    "    for idx in range(1,num_vec+1):\n",
    "        his_vec = np.zeros(output_size)#output_size=11999\n",
    "        # np.power(.9,15-iterater)  #counts down \n",
    "        decayed_val = np.power(within_decay_rate,num_vec-idx)\n",
    "        #elements in data_set['2']\n",
    "        for ele in vec_list[idx]:\n",
    "            his_vec[ele] = decayed_val\n",
    "        his_list.append(his_vec)\n",
    "        \n",
    "    grouped_vec_list = []\n",
    "    if len(his_list) < group_size:\n",
    "        #sum = np.zeros(len(his_list[0]))\n",
    "        for j in range(len(his_list)):\n",
    "            grouped_vec_list.append(his_list[j])\n",
    "\n",
    "        grouped_list,real_group_size = grouped_vec_list, len(his_list)\n",
    "    else:\n",
    "        est_num_vec_each_block = len(his_list)/group_size\n",
    "        base_num_vec_each_block = int(np.floor(len(his_list)/group_size))\n",
    "        residual = est_num_vec_each_block - base_num_vec_each_block\n",
    "\n",
    "        num_vec_has_extra_vec = int(np.round(residual * group_size))\n",
    "\n",
    "        if residual == 0:\n",
    "            for i in range(group_size):\n",
    "                if len(his_list)<1:\n",
    "                    print('len(his_list)<1')\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "        else:\n",
    "            for i in range(group_size - num_vec_has_extra_vec):\n",
    "                sum = np.zeros(len(his_list[0]))\n",
    "                for j in range(base_num_vec_each_block):\n",
    "                    if i*base_num_vec_each_block+j >= len(his_list):\n",
    "                        print('i*base_num_vec_each_block+j')\n",
    "                    sum += his_list[i*base_num_vec_each_block+j]\n",
    "                    last_idx = i * base_num_vec_each_block + j\n",
    "                grouped_vec_list.append(sum/base_num_vec_each_block)\n",
    "\n",
    "            est_num = int(np.ceil(est_num_vec_each_block))\n",
    "            start_group_idx = group_size - num_vec_has_extra_vec\n",
    "            if len(his_list) - start_group_idx*base_num_vec_each_block >= est_num_vec_each_block:\n",
    "                for i in range(start_group_idx,group_size):\n",
    "                    sum = np.zeros(len(his_list[0]))\n",
    "                    for j in range(est_num):\n",
    "                        # if residual+(i-1)*est_num_vec_each_block+j >= len(his_list):\n",
    "                        #     print('residual+(i-1)*num_vec_each_block+j')\n",
    "                        #     print('len(his_list)')\n",
    "                        iidxx = last_idx + 1+(i-start_group_idx)*est_num+j\n",
    "                        if  iidxx >= len(his_list) or iidxx<0:\n",
    "                            print('last_idx + 1+(i-start_group_idx)*est_num+j')\n",
    "                        sum += his_list[iidxx]\n",
    "                    grouped_vec_list.append(sum/est_num)\n",
    "\n",
    "        grouped_list,real_group_size = grouped_vec_list, group_size\n",
    "    his_vec = np.zeros(output_size)\n",
    "    for idx in range(real_group_size):\n",
    "        decayed_val = np.power(group_decay_rate, group_size - 1 - idx)\n",
    "        if idx>=len(grouped_list):\n",
    "            print( 'idx: '+ str(idx))\n",
    "            print('len(grouped_list): ' + str(len(grouped_list)))\n",
    "        his_vec += grouped_list[idx]*decayed_val\n",
    "    sum_history[key] = his_vec/real_group_size\n",
    "    # sum_history[key] = np.multiply(his_vec / real_group_size, IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_decay_sum_history_test = sum_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_set=temporal_decay_sum_history_test \n",
    "target_set=temporal_decay_sum_history_training \n",
    "k=num_nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding k nearest neighbors...\n",
      "Finish KNN search.\n"
     ]
    }
   ],
   "source": [
    "#def KNN(query_set, target_set, k):\n",
    "history_mat = []\n",
    "for key in target_set.keys():\n",
    "    history_mat.append(target_set[key])\n",
    "test_mat = []\n",
    "for key in query_set.keys():\n",
    "    test_mat.append(query_set[key])\n",
    "print('Finding k nearest neighbors...')\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(history_mat)\n",
    "distances, indices = nbrs.kneighbors(test_mat)\n",
    "print('Finish KNN search.' )\n",
    "index, distance = indices,distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_history_test,test_key_set,training_sum_history_test,training_key_set,index,alpha = temporal_decay_sum_history_test, test_key_set, temporal_decay_sum_history_training,training_key_set, index, alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_history = {}\n",
    "for test_key_id in range(len(test_key_set)):\n",
    "    test_key = test_key_set[test_key_id]\n",
    "    test_history = sum_history_test[test_key]\n",
    "    sum_training_history = np.zeros(len(test_history))\n",
    "    for indecis in index[test_key_id]:\n",
    "        training_key = training_key_set[indecis]\n",
    "        sum_training_history += training_sum_history_test[training_key]\n",
    "\n",
    "    sum_training_history = sum_training_history/len(index[test_key_id])\n",
    "\n",
    "    merge = test_history*alpha + sum_training_history*(1-alpha)\n",
    "    merged_history[test_key] = merge\n",
    "    \n",
    "sum_history = merged_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision of : 0.053364519611371 with std: 0.07979832834575636\n",
      "recall:  0.1296420382929529\n",
      "NDCG:  0.09875611529474698\n"
     ]
    }
   ],
   "source": [
    "if activate_codes_num < 0:\n",
    "    # for i in range(1, 6):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    F = []\n",
    "    prec1 = []\n",
    "    rec1 = []\n",
    "    F1 = []\n",
    "    prec2 = []\n",
    "    rec2 = []\n",
    "    F2 = []\n",
    "    prec3 = []\n",
    "    rec3 = []\n",
    "    F3 = []\n",
    "    NDCG = []\n",
    "    n_hit = 0\n",
    "\n",
    "    num_ele = topk\n",
    "    # print('k = ' + str(activate_codes_num))\n",
    "    # evaluate(data_chunk, input_size,test_KNN_history, test_key_set, next_k_step)\n",
    "    count = 0\n",
    "    for iter in range(len(test_key_set)):\n",
    "        \n",
    "        # training_pair = training_pairs[iter - 1]\n",
    "        # input_variable = training_pair[0]\n",
    "        # target_variable = training_pair[1]\n",
    "        input_variable = data_chunk[training_chunk][test_key_set[iter]]\n",
    "        target_variable = data_chunk[test_chunk][test_key_set[iter]]\n",
    "\n",
    "        if len(target_variable) < 2 + next_k_step:\n",
    "            continue\n",
    "        count += 1\n",
    "        sum_history,key = sum_history, test_key_set[iter]\n",
    "        #def predict_with_elements_in_input(sum_history,key):\n",
    "        output_vectors = []\n",
    "\n",
    "        for idx in range(next_k_step):\n",
    "            vec = sum_history[key]\n",
    "            output_vectors.append(vec)\n",
    "        output_vectors = output_vectors        \n",
    "        top = 400\n",
    "        hit = 0\n",
    "        for idx in range(len(output_vectors)):\n",
    "            # for idx in [2]:\n",
    "\n",
    "            output = np.zeros(input_size)\n",
    "            target_topi = output_vectors[idx].argsort()[::-1][:top]\n",
    "            c = 0\n",
    "            for i in range(top):\n",
    "                if c >= num_ele:\n",
    "                    break\n",
    "                output[target_topi[i]] = 1\n",
    "                c += 1\n",
    "\n",
    "            vectorized_target = np.zeros(input_size)\n",
    "            for ii in target_variable[1 + idx]:\n",
    "                vectorized_target[ii] = 1\n",
    "            groundtruth,pred = vectorized_target, output\n",
    "            #def get_precision_recall_Fscore(groundtruth,pred):\n",
    "            a = groundtruth\n",
    "            b = pred\n",
    "            correct = 0\n",
    "            truth = 0\n",
    "            positive = 0\n",
    "\n",
    "            for idx in range(len(a)):\n",
    "                if a[idx] == 1:\n",
    "                    truth += 1\n",
    "                    if b[idx] == 1:\n",
    "                        correct += 1\n",
    "                if b[idx] == 1:\n",
    "                    positive += 1\n",
    "\n",
    "            flag = 0\n",
    "            if 0 == positive:\n",
    "                precision = 0\n",
    "                flag = 1\n",
    "                #print('postivie is 0')\n",
    "            else:\n",
    "                precision = correct/positive\n",
    "            if 0 == truth:\n",
    "                recall = 0\n",
    "                flag = 1\n",
    "                #print('recall is 0')\n",
    "            else:\n",
    "                recall = correct/truth\n",
    "\n",
    "            if flag == 0 and precision + recall > 0:\n",
    "                G = 2*precision*recall/(precision+recall)\n",
    "            else:\n",
    "                G = 0\n",
    "            precision, recall, Fscore, correct = precision, recall, G, correct            \n",
    "            prec.append(precision)\n",
    "            rec.append(recall)\n",
    "            F.append(Fscore)\n",
    "            if idx == 0:\n",
    "                prec1.append(precision)\n",
    "                rec1.append(recall)\n",
    "                F1.append(Fscore)\n",
    "            elif idx == 1:\n",
    "                prec2.append(precision)\n",
    "                rec2.append(recall)\n",
    "                F2.append(Fscore)\n",
    "            elif idx == 2:\n",
    "                prec3.append(precision)\n",
    "                rec3.append(recall)\n",
    "                F3.append(Fscore)\n",
    "            groundtruth, pred_rank_list,k=vectorized_target, target_topi, num_ele\n",
    "            #def get_HT(groundtruth, pred_rank_list,k):\n",
    "            count = 0\n",
    "            for pred in pred_rank_list:\n",
    "                if count >= k:\n",
    "                    break\n",
    "                if groundtruth[pred] == 1:\n",
    "                    hit += 1\n",
    "                count += 1\n",
    "            hit += 0\n",
    "            #input_size = 100\n",
    "            vectorized_target, target_topi, num_ele=groundtruth, pred_rank_list,k\n",
    "            #def get_NDCG1(groundtruth, pred_rank_list,k):\n",
    "            count = 0\n",
    "            dcg = 0\n",
    "            for pred in pred_rank_list:\n",
    "                if count >= k:\n",
    "                    break\n",
    "                if groundtruth[pred] == 1:\n",
    "                    dcg += (1)/math.log2(count+1+1)\n",
    "                count += 1\n",
    "            idcg = 0\n",
    "            num_real_item = np.sum(groundtruth)\n",
    "            num_item = int(num_real_item)\n",
    "            for i in range(num_item):\n",
    "                idcg += (1) / math.log2(i + 1 + 1)\n",
    "            ndcg = dcg / idcg\n",
    "            NDCG.append(ndcg)\n",
    "        if hit == next_k_step:\n",
    "            n_hit += 1\n",
    "            \n",
    "    print('average precision of ' + ': ' + str(np.mean(prec)) + ' with std: ' + str(np.std(prec)))\n",
    "    recall = np.mean(rec)\n",
    "    ndcg = np.mean(NDCG)\n",
    "    hr = n_hit / len(test_key_set)\n",
    "    \n",
    "recall, ndcg, hr = recall, ndcg, hr\n",
    "print('recall: ', str(recall))\n",
    "print('NDCG: ', str(ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
